{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please enter your email to access the NCBI Entrez records (useful for getting the species name to which each protein belongs to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrez_email = 'fp1n17@soton.ac.uk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, sys, os, collections, itertools, holoviews as hv, multiprocessing\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV,train_test_split\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score, roc_curve\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from Bio import SeqIO\n",
    "from Bio import Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GENERATE INDICES FOR CONSTRUCTING TRAIN-TEST SET PAIRS FOR EACH SPECIES IN LOBOV'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''GENERATE INDICES FOR CONSTRUCTING TRAIN-TEST SET PAIRS FOR EACH SPECIES IN LOBOV'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''setting data Paths'''\n",
    "prots_path='./Supplementary_Table_2.xlsx'\n",
    "trigrams_path='./protVec_100d_3grams.csv'\n",
    "\n",
    "tut_path = 'tut-3-lobov'\n",
    "d_path = os.path.join(tut_path,'bpad200_lobov_splits_indic')\n",
    "if os.path.exists(d_path) == False :\n",
    "    os.makedirs(d_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1DI0_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1DYQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2Y1V_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A46405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAA20874.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>YP_222757.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>YP_695964.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>YP_696046.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>YP_763859.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>YP_884378.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id\n",
       "0         1DI0_A\n",
       "1           1DYQ\n",
       "2         2Y1V_C\n",
       "3         A46405\n",
       "4     AAA20874.1\n",
       "..           ...\n",
       "395  YP_222757.1\n",
       "396  YP_695964.1\n",
       "397  YP_696046.1\n",
       "398  YP_763859.1\n",
       "399  YP_884378.1\n",
       "\n",
       "[400 rows x 1 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''get species of each protein from NCBI'''\n",
    "\n",
    "prots_data = pd.read_excel(prots_path\n",
    "                             ,usecols = ['Id'],index_col=None\n",
    "                            ,skiprows=[0,1])\n",
    "\n",
    "\n",
    "prots_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein species name collected:  0    Current protein collected:  1DI0_A \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stelios\\anaconda3\\envs\\Frixos\\lib\\site-packages\\Bio\\GenBank\\__init__.py:1139: BiopythonParserWarning: Dropping bond qualifier in feature location\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''Cell has large time complexity (around 5 mins to run)'''\n",
    "\n",
    "#load the ijms-18-00312-s001.fa file to get the sequence of each protein\n",
    "#TODO: 'ijms-18-00312-s001.fa' does not have sequences for all proteins...!!!!\n",
    "#and also has different accession names than NCBI protein db (extra/missing '.' , '_' and other characters)\n",
    "\n",
    "#get the sequences from NCBI \n",
    "#check 'Entrez Guidelines' http://biopython.org/DIST/docs/tutorial/Tutorial.html#chapter%3Aentrez\n",
    "#code ref: https://www.ncbi.nlm.nih.gov/dbvar/content/tools/entrez/\n",
    "species_names=[]\n",
    "Entrez.email  = entrez_email\n",
    "db = 'Protein'\n",
    "\n",
    "for i,acc in enumerate(prots_data.loc[:,'Id'].values.tolist()):\n",
    "    if i%50==0:\n",
    "        print('Protein species name collected: ',i,'   Current protein collected: ',acc,'\\n')\n",
    "    #testing\n",
    "    #print(species_names,'\\n',acc,'\\n')\n",
    "    try:\n",
    "        handle = Entrez.efetch(db=\"protein\", id=acc, rettype=\"gb\", retmode=\"text\")\n",
    "    except Exception:\n",
    "        if '_' not in acc:\n",
    "            acc = acc+'_A' \n",
    "            try:\n",
    "                handle = Entrez.efetch(db=\"protein\", id=acc, rettype=\"gb\", retmode=\"text\")\n",
    "            except Exception:\n",
    "                if '.' not in acc:\n",
    "                    acc = acc[:-2]+'.1'\n",
    "                    handle = Entrez.efetch(db=\"protein\", id=acc, rettype=\"gb\", retmode=\"text\")\n",
    "    \n",
    "    record = SeqIO.read(handle, \"genbank\")\n",
    "    handle.close()\n",
    "    \n",
    "    species_names.append(str(record.annotations['organism']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#species_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create proteins dataset and store in the desired format for use later on\n",
    "\n",
    "prots_data = pd.concat([prots_data.iloc[:,0],pd.DataFrame(species_names,columns = ['Species'])],axis=1)\n",
    "#prots_data.columns=cols_\n",
    "prots_data.rename({'Id':'Accession'},inplace=True,axis=1)\n",
    "prots_data.to_csv(tut_path+'\\\\bpad200_species.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''function handling generation of test set indices for LOBOV'''\n",
    "\n",
    "def store_lobov_splits(dataset_df,acc_spec_df,thresh):\n",
    "    '''function that takes full @dataset_df of proteins and separately accessions_species df @acc_spec_df \n",
    "    for the same proteins as arguments, returns the train test splits for outer CV, \n",
    "    where each test-set contains proteins from the same bacterial species \n",
    "    (this was only done where we have more than @thresh proteins overall for that species)'''\n",
    "    '''this fn is found useful in cases where we already have the original dataset\n",
    "    but without the species, which we receive at some later point in the proj'''\n",
    "    '''@acc_spec_df should have species as index when passed to this df'''\n",
    "    \n",
    "    species_col = []\n",
    "    \n",
    "    if set(dataset['Accession'].values.tolist())!=set(acc_spec['Accession'].values.tolist()):\n",
    "        # apply acc.split('.')[0] to have equal set(dataset['Accession']) and set(acc_spec['Accession'])\n",
    "        dataset['Accession'].apply(lambda x: x.split('.')[0]   )\n",
    "        acc_spec['Accession'].apply(lambda x: x.split('.')[0])\n",
    "    \n",
    "    #get species for each protein\n",
    "    for acc in dataset_df['Accession']:\n",
    "        species_col.append(acc_spec[acc_spec['Accession']==acc]['Species'].values[0]) \n",
    "    \n",
    "    #take each protein's species, keep only the first 2 words in the string specifying the species\n",
    "    #,remove any double whitespace (of any kind, even tab characters) , remove trailing whitespaces on both sides\n",
    "    species_col = [' '.join(s.split()[:2]).lstrip().rstrip() for s in species_col]\n",
    "    \n",
    "    #where there are ambiguous names, such as with Borrelia burgdorferi, Chlamydophila pneumoniae, we kept the names used by Heinson et. al 2017\n",
    "    species_col_ = []\n",
    "    for species in species_col:\n",
    "        if species=='Borreliella burgdorferi':\n",
    "            species = 'Borrelia burgdorferi'\n",
    "        elif species=='Chlamydia pneumoniae':\n",
    "            species = 'Chlamydophila pneumoniae'\n",
    "        \n",
    "        species_col_.append(species)\n",
    "    \n",
    "    #testing\n",
    "    #print(species_col_)\n",
    "    #input()\n",
    "    \n",
    "    #assert len(set(species_col))==42\n",
    "    \n",
    "    dataset_df['Species'] = species_col_\n",
    "    \n",
    "    count_spec = Counter(species_col_)\n",
    "    lobov_spec = [s for s in count_spec if count_spec[s] >4 ]\n",
    "    \n",
    "    #print((lobov_spec),'\\n')\n",
    "    #input()\n",
    "    \n",
    "    assert len(lobov_spec)==25\n",
    "    \n",
    "    s_dir = os.path.join(tut_path,'bpad200_lobov_splits_indic')\n",
    "    if os.path.exists('bpad200_lobov_splits_indic') == False : \n",
    "        os.mkdir('bpad200_lobov_splits_indic')\n",
    "    \n",
    "    \n",
    "    for spec in lobov_spec:\n",
    "        test_indic = dataset_df[dataset_df['Species']==spec].index.tolist()\n",
    "        train_indic = [i for i in range(400) if i not in test_indic]\n",
    "\n",
    "        df = pd.DataFrame([test_indic,train_indic]).T\n",
    "        df.columns = ['Test','Train']\n",
    "        \n",
    "        #testing\n",
    "        #input()\n",
    "        \n",
    "        #store training/test indic in file for each species\n",
    "        df.to_csv(s_dir+'\\\\'+spec+'.csv',index=None)\n",
    "     \n",
    "    \n",
    "    print('\\n')\n",
    "    print([(s,count_spec[s]) for s in count_spec if count_spec[s] >4 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load initial proteins dataset'''\n",
    "\n",
    "prots_path = './BPAD200.csv'\n",
    "\n",
    "'''load proteins'''\n",
    "#load bpad200 proteins\n",
    "dataset = pd.read_csv(prots_path\n",
    "                      ,index_col=None,usecols=['Accession','Sequence','Label'])\n",
    "#dataset.reset_index(inplace=True,drop=False)\n",
    "\n",
    "# handling empty lines between protein records dataset given\n",
    "if len(dataset)>400:\n",
    "    dataset= dataset[1::2]\n",
    "    dataset.reset_index(drop=False,inplace=True)\n",
    "\n",
    "# testing\n",
    "assert len(dataset)==400\n",
    "\n",
    "'''load accessions-species '''\n",
    "acc_spec = pd.read_csv(tut_path+'\\\\bpad200_species.csv',index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''code-block to get the files with the indices of test set for each species'''\n",
    "threshold=4\n",
    "#if os.path.exists('./bpad200_lobov_splits_indic.csv')==False:\n",
    "\n",
    "acc_spec.reset_index(inplace=True,drop=False)\n",
    "#acc_spec\n",
    "store_lobov_splits(dataset,acc_spec,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''UTILITY CLASS'''\n",
    "\n",
    "\n",
    "class Load_data_model_selection():\n",
    "    def __init__(self,prot_path=None,prot_cols=None,prot_ind=None):\n",
    "        #print('\\nRefactor function below to work on class instances variables and convert clas to inherit from a more generic load_data class, which class will be parent for all load_data classes for all stages in proj pipeline\\n')\n",
    "        \n",
    "        self.prot_cols = prot_cols\n",
    "        self.prot_ind = prot_ind\n",
    "        self.data_df={}\n",
    "        \n",
    "\n",
    "    def get_all_data(self):\n",
    "        return self.data_df\n",
    "\n",
    "        \n",
    "    def populate_data_df(self,all_paths): \n",
    "        if isinstance(all_paths,list)==False:\n",
    "            all_paths = [all_paths]\n",
    "        \n",
    "        for data_path in all_paths:\n",
    "            ##debugging\n",
    "            ##print('\\n',split_fully(data_path),'\\n',data_path,'\\n')\n",
    "            self.data_df.setdefault('preProcessing_1',[]).append( split_fully(data_path)[-3] )\n",
    "            self.data_df.setdefault('preProcessing_2',[]).append( split_fully(data_path)[-2] )\n",
    "            \n",
    "            self.data_df.setdefault('name',[]).append( os.path.basename(data_path).split('.')[0] )\n",
    "            self.data_df.setdefault('dataset',[]).append( self.load_prot_vecs(prot_path = data_path) )        \n",
    "\n",
    "        self.data_df = pd.DataFrame(self.data_df)\n",
    "        \n",
    "        \n",
    "    def load_prot_vecs(self,prot_path,cols=None,ind=None):\n",
    "        if ind!=None and cols!=None:\n",
    "            self.prot_cols = cols\n",
    "            self.prot_ind=ind\n",
    "        \n",
    "        if self.prot_cols=='infer':\n",
    "            prot_vecs = pd.read_csv(prot_path,\n",
    "                               header=self.prot_cols,\n",
    "                               index_col=self.prot_ind,)\n",
    "                               #skip_blank_lines= True)\n",
    "        elif isinstance(self.prot_cols,list):\n",
    "            prot_vecs= pd.read_csv(prot_path,\n",
    "                               usecols=self.prot_cols,\n",
    "                               index_col=self.prot_ind,)\n",
    "        \n",
    "        \n",
    "        return prot_vecs\n",
    "    \n",
    "\n",
    "def get_dict_key_ind(dict_keys,key_name):\n",
    "    '''fn that returns the index of the @key_name arg\n",
    "    in the @keys_list of a dict_ (dict_ present at code block where this fn is called)\n",
    "    Particularly useful when we want to get value of dict entry by key name, when\n",
    "    we are iterating over entries of dict_ (not possible otherwise)\n",
    "    '''\n",
    "    dict_keys = [k for k in dict_keys]\n",
    "    ind = dict_keys.index(key_name)\n",
    "    \n",
    "    return ind\n",
    "\n",
    "    '''a note on iterating dictionaries by row (as we conceptually think dfs):\n",
    "        don't use .items(), it returns (key,value) pair for each key (ie: 4 tuples if we have for keys in dict)\n",
    "    '''\n",
    "       \n",
    "    \n",
    "def split_fully(path):\n",
    "    '''split a windows path to all its parts (folders,files)'''\n",
    "    #ref: https://www.oreilly.com/library/view/python-cookbook/0596001673/ch04s16.html\n",
    "    allparts = []\n",
    "    while 1:\n",
    "        parts = os.path.split(path)\n",
    "        if parts[0] == path:  # sentinel for absolute paths\n",
    "            allparts.insert(0, parts[0])\n",
    "            break\n",
    "        elif parts[1] == path: # sentinel for relative paths\n",
    "            allparts.insert(0, parts[1])\n",
    "            break\n",
    "        else:\n",
    "            path = parts[0]\n",
    "            allparts.insert(0, parts[1])\n",
    "    return allparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''UTILITY CLASS'''\n",
    "\n",
    "\"\"\"\n",
    "utility class for classification\n",
    "-- this class handles just 1 model instance at a time\n",
    "\"\"\"\n",
    "\n",
    "class Classifier_model():\n",
    "    def __init__(self,method=None,data_name=None,dataset=None,dataset_path=None,\n",
    "                 preProcessing_1=None,preProcessing_2=None,split_method=None\n",
    "                 ,outer_splits=None,inner_splits=None,hparams_model=None):\n",
    "        RSEED=10\n",
    "        \n",
    "        self.best_run_details={}\n",
    "        np.random.seed(RSEED)\n",
    "        plt.ioff()\n",
    "        \n",
    "        self.method=method\n",
    "        self.data_name=data_name\n",
    "        self.dataset=dataset\n",
    "        self.dataset_path=dataset_path\n",
    "        self.preProcessing_1=preProcessing_1\n",
    "        self.preProcessing_2=preProcessing_2\n",
    "        self.split_method=split_method\n",
    "        self.outer_splits=outer_splits\n",
    "        self.inner_splits=inner_splits\n",
    "        self.hparams_model = hparams_model\n",
    "        \n",
    "        self.split_method_path = make_dir([self.dataset_path,self.split_method])\n",
    "        \n",
    "        #tut-3-lobov\\standardise\\PCA\\PCs\\lobov\\LR\n",
    "        self.classif_method_path = make_dir([self.split_method_path,self.method])\n",
    "    \n",
    "      \n",
    "    def get_run_details(self):\n",
    "        return self.best_run_details\n",
    "        \n",
    "    def get_features(self,dataset):\n",
    "        return dataset.iloc[:, 2:-1]\n",
    "        \n",
    "    def get_labels(self,dataset):\n",
    "        return dataset.iloc[:, -1]    \n",
    "    \n",
    "    \n",
    "    def outer_cv(self,lobov_data_dir):\n",
    "        outer_runs_details = {'fold_id':[],'test_auc':[],'best_inner_hparams':[]}\n",
    "        \n",
    "        if self.split_method=='lobov':\n",
    "            print('\\n--------------'+self.split_method.upper()+' has started..\\n')\n",
    "            \n",
    "            #each file in lobov dir corresponds to indices for train data\n",
    "            #and test data, where test data comprises of proteins from only 1 bact.species\n",
    "            files_ = os.listdir(lobov_data_dir)\n",
    "            for file_ in files_:\n",
    "                file_path = os.path.join(lobov_data_dir,file_)\n",
    "                species = file_.split('.csv')[0]\n",
    "                \n",
    "                best_inner_run_details = self.outer_cv_fold_specie(species,file_path)\n",
    "                                                \n",
    "                #saving details for this fold in the outer folds dict\n",
    "                #inner_runs_details contains best_inner_hparams,test_auc\n",
    "                outer_runs_details['fold_id'].append(species)\n",
    "                for key in best_inner_run_details:\n",
    "                    outer_runs_details[key].append(best_inner_run_details[key])\n",
    "        \n",
    "        \n",
    "        elif self.split_method=='random':\n",
    "            print('\\n'+self.split_method+' OUTER '+str(self.outer_splits)+'-FOLD CV has started..')\n",
    "            \n",
    "            cv = StratifiedKFold(n_splits=self.outer_splits,shuffle=True,random_state=10)\n",
    "        \n",
    "            for fold_id,(train_indic, test_indic) in enumerate(cv.split(self.get_features(self.dataset),self.get_labels(self.dataset))):\n",
    "                \n",
    "                fold_path = make_dir([self.classif_method_path,'fold_'+str(fold_id)])\n",
    "                best_inner_run_details=self.inner_cv(train_indic,test_indic\n",
    "                                                    ,fold_id=fold_id,fold_path=fold_path)\n",
    "        \n",
    "                \n",
    "                outer_runs_details['fold_id'].append(fold_id)\n",
    "                for key in best_inner_run_details:\n",
    "                    outer_runs_details[key].append(best_inner_run_details[key])\n",
    "            \n",
    "        \n",
    "        #eval outer cv \n",
    "        print('\\n***Evaluating models\\' performance on testing data during C.V.')\n",
    "        avg_test_auc,best_hparams,test_auc_std = self.eval_outer_cv(outer_runs_details)\n",
    "        \n",
    "        ###############print('\\n**Average test AUC score: ',avg_test_auc)\n",
    "        \n",
    "        #ref=  https://stackoverflow.com/questions/633127/viewing-all-defined-variables/633134\n",
    "        self.best_run_details = {'preProcessing_1':self.preProcessing_1,\n",
    "                                 'preProcessing_2':self.preProcessing_2,\n",
    "                                 'data_name':self.data_name,\n",
    "                                 'split_method':self.split_method,\n",
    "                                 'outer_splits':self.outer_splits,\n",
    "                                 'inner_splits':self.inner_splits,\n",
    "                                 'method':self.method,\n",
    "                                 'avg_test_auc':avg_test_auc,\n",
    "                                 'test_auc_std':test_auc_std,\n",
    "                                 'best_hparams':best_hparams}\n",
    "        \n",
    "        \n",
    "    def outer_cv_fold_specie(self,species,file_path):\n",
    "        print('\\nGathering data for bacteria ',species)\n",
    "        fold_path = make_dir([self.classif_method_path,species])\n",
    "        \n",
    "        splits_indic = pd.read_csv(file_path,index_col=None)\n",
    "        \n",
    "        #use indices from file to split dataset to train and test\n",
    "        train_indic = list(splits_indic.loc[:,'Train'].values)\n",
    "        test_indic = (splits_indic.loc[:,'Test'].dropna(axis=0,inplace=False).values).tolist()\n",
    "        test_indic = [int(j) for j in test_indic]\n",
    "         \n",
    "        best_inner_run_details = self.inner_cv(train_indic,test_indic,fold_id=species,fold_path=fold_path)\n",
    "        \n",
    "        return best_inner_run_details \n",
    "    \n",
    "        \n",
    "    def inner_cv(self,train_indic,test_indic,fold_id,fold_path):\n",
    "        \n",
    "        best_inner_run_details= self.classifier_model_fold_run(train_indic,test_indic,fold_id,fold_path)\n",
    "        \n",
    "        return best_inner_run_details\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def classifier_model_fold_run(self,train_indic,test_indic,fold_id,fold_path):\n",
    "        '''classifier with inner C.V. for hyperparam. tuning\n",
    "        the basic results are stored and visualized (ROC_AUC score, accuracy)\n",
    "        '''\n",
    "        #ref: #ref: https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/Random%20Forest%20Tutorial.ipynb\n",
    "        # reference:  https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n",
    "        # reference: https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python\n",
    "        \n",
    "        train_features = self.get_features(self.dataset.iloc[train_indic,:])\n",
    "        train_labels = self.get_labels(self.dataset.iloc[train_indic,:])\n",
    "        test_features = self.get_features(self.dataset.iloc[test_indic,:])\n",
    "        test_labels = self.get_labels(self.dataset.iloc[test_indic,:])\n",
    "        \n",
    "        #TODO: EDA\n",
    "        \n",
    "       \n",
    "        print('\\n\\nStarting GridSearch hyper-parameter tuning on '+self.method+' with '+(self.split_method)+' and fold_id='+str(fold_id)+\n",
    "                ',  10-fold inner C.V. on  '+self.data_name+'  dataset with/from: '+self.preProcessing_1+'-'+self.preProcessing_2+'\\n')\n",
    "        classifier = self.build_classifier()\n",
    "        \n",
    "        #cv instance created within GridSearchCV instance during excecution, has parameter\n",
    "        #shuffle=False => no randomness present => no need to pass randomSeed val.\n",
    "        #n_jobs=-1 may cause memory error iussues\n",
    "        #NOTE! : You cannot nest objects with parallel computing (n_jobs different than 1).\n",
    "        n_jobs=multiprocessing.cpu_count()\n",
    "        grid_search = GridSearchCV(estimator = classifier, param_grid=self.hparams_model,\n",
    "                                   scoring='roc_auc',n_jobs = -1, #computing_server has 6 cores, 12 threads  \n",
    "                        cv = self.inner_splits, refit=True, \n",
    "                        verbose = 1)#,pre_dispatch=2*n_jobs)\n",
    "        \n",
    "        grid_search.fit(train_features,train_labels)\n",
    "        \n",
    "        print('\\nFinished hyperparameter tuning of '+self.method+' on dataset\\n')\n",
    "        best_inner_hparams = grid_search.best_params_\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        print('\\nMaking predictions with the best model..\\n')\n",
    "        train_predictions = best_model.predict(train_features)\n",
    "        #train_svm_probs = float64 array, (shape=#train_samples x 1), col2='probab to be BPA' (label=1='the greater label/positive class'), col1 = '..to be nonBPA'\n",
    "        train_probs = best_model.predict_proba(train_features)[:, 1]\n",
    "        #test_predictions:: array\n",
    "        test_predictions = best_model.predict(test_features)\n",
    "        test_probs = best_model.predict_proba(test_features)[:, 1]\n",
    "        \n",
    "        #save_predictions_classifier(self,'train',train_indic,train_predictions,train_probs,fold_path)\n",
    "        #save_predictions_classifier(self,'test',test_indic,test_predictions,test_probs,fold_path)\n",
    "        \n",
    "        print('\\nEvaluating basic metrics on model performance\\n')\n",
    "        test_auc = self.visualise_evaluate(test_labels,test_predictions, test_probs,fold_path)\n",
    "\n",
    "        #we don't re-use the c.v. models for final model deployment => no need to save model below\n",
    "        #pickle.dump(best_model,open(fold_path+'\\\\best_model.pkl','wb'))\n",
    "        \n",
    "        #plot feature importance and save it\n",
    "        if self.method=='XGBoost':\n",
    "            #TODO: make chart bigger, have to pass ax object here ..\n",
    "            xgb.plot_importance(best_model)\n",
    "           # plt.savefig(fold_path+'\\\\feature_importance.png')\n",
    "        elif self.method=='RF':\n",
    "            fi_model = pd.DataFrame({'feature': self.get_features(self.dataset),\n",
    "                   'importance': best_model.feature_importances_}).\\\n",
    "                    sort_values('importance', ascending = False)\n",
    "            #fi_model.to_csv(fold_path+'\\\\feature_importance.csv')\n",
    "\n",
    "\n",
    "        return {'test_auc':test_auc, 'best_inner_hparams':best_inner_hparams}\n",
    "    \n",
    "    \n",
    "\n",
    "    def build_classifier(self):\n",
    "        '''GridSearchCV with passed hparams overwrites hparams set here in model obj.defn'''\n",
    "        if self.method=='SVM':\n",
    "            #if SVM is slow, search for trick with probability=False & predict_proba stackPOverlfow\n",
    "            classifier = svm.SVC(degree =1, coef0=0.0, probability=True,\n",
    "                            verbose=False,random_state=10,**self.hparams_model)\n",
    "        \n",
    "        elif self.method=='XGBoost':\n",
    "            classifier=xgb.XGBClassifier(objective='binary:logistic',eval_metric='auc',\n",
    "          nthread=1, colsample_bytree=1, subsample=1,colsample_bylevel=0.5,\n",
    "              nrounds=1,random_state=10,**self.hparams_model)\n",
    "            \n",
    "        elif self.method=='LR':\n",
    "            classifier = LogisticRegression(penalty='l2',solver='liblinear',\n",
    "                                            max_iter=100,random_state=10,**(self.hparams_model))\n",
    "        \n",
    "        elif self.method=='RF':\n",
    "            classifier = RandomForestClassifier(random_state=10,**self.hparams_model)\n",
    "            \n",
    "        return classifier\n",
    "    \n",
    "    \n",
    "    def run_finalise(self):\n",
    "        '''for finalising (training+testing) model after model_selection'''\n",
    "        print('\\n\\n**Retrieved best model and hyper-parameters from model selection and started finalizing..')    \n",
    "        \n",
    "        features = self.get_features(self.dataset)\n",
    "        labels = self.get_labels(self.dataset)\n",
    "        \n",
    "        classifier = self.build_classifier()\n",
    "        \n",
    "        print('\\nStarted training of '+self.method+' on FULL dataset'+self.data_name+'\\n')\n",
    "        \n",
    "        classifier.fit(features,labels)\n",
    "        \n",
    "        print('\\nFinished training of '+self.method+' on FULL dataset\\n')\n",
    "        \n",
    "        print('\\nMaking predictions with the best model..\\n')\n",
    "        predictions = classifier.predict(features)\n",
    "        #train_svm_probs = float64 array, (shape=#train_samples x 1), col2='probab to be BPA' (label=1='the greater label/positive class'), col1 = '..to be nonBPA'\n",
    "        probs = classifier.predict_proba(features)[:, 1]\n",
    "        \n",
    "        indic = list(np.arange(len(self.dataset)))\n",
    "        save_predictions_classifier(self,'finalised',indic,predictions,\n",
    "                                    probs,self.classif_method_path)\n",
    "        \n",
    "        print('\\nEvaluating basic metrics on model performance\\n')\n",
    "        test_auc = self.visualise_evaluate(labels,predictions, probs,self.classif_method_path)\n",
    "\n",
    "        #pickle.dump(classifier,open(self.classif_method_path+'\\\\best_model.pkl','wb'))\n",
    "        \n",
    "        self.plot_feature_importance(classifier)\n",
    "        \n",
    "    \n",
    "        self.best_run_details = {'preProcessing_1':self.preProcessing_1,\n",
    "                                 'preProcessing_2':self.preProcessing_2,\n",
    "                                 'data_name':self.data_name,\n",
    "                                 'split_method':self.split_method,\n",
    "                                 'outer_splits':self.outer_splits,\n",
    "                                 'inner_splits':self.inner_splits,\n",
    "                                 'method':self.method,\n",
    "                                 'test_auc':test_auc,\n",
    "                                 'best_hparams':self.hparams_model}\n",
    "    \n",
    "        \n",
    "    \n",
    "    def visualise_evaluate(self,labels,predictions, probs,fold_path):\n",
    "        #probs, train_probs : array size=(samples x 1),, contains probability that each prot is BPA (the positive class), as returned by model\n",
    "        #ref: https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/Random%20Forest%20Tutorial.ipynb\n",
    "        \"\"\"get stats and ROC-AUC on training and testing data\n",
    "        suitable ONLY for BINARY classification and balanced classes!\"\"\"\n",
    "        \n",
    "        baseline = {}\n",
    "        \n",
    "        baseline['accuracy']=0.5\n",
    "        baseline['roc_auc'] = 0.5\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        #y_score=y_probs=  probability estimates of the positive class\n",
    "        results['roc_auc'] = roc_auc_score(labels, y_score=probs)\n",
    "        results['accuracy']=accuracy_score(labels, predictions)\n",
    "        \n",
    "        train_results = {}\n",
    "        \n",
    "        for metric in ['accuracy', 'roc_auc']:\n",
    "            print(f'{metric.capitalize()}, Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)}')\n",
    "        \n",
    "        # Calculate false positive rates and true positive rates, AUC\n",
    "        base_fpr, base_tpr, _ = roc_curve(labels, [1 for _ in range(len(labels))])\n",
    "        model_fpr, model_tpr, test_thresholds = roc_curve(labels, probs)\n",
    "        ## dont calc AUC like below, cause it's more general and not the specific way you should do it for the AUC of an ROC curve\n",
    "        #model_auc = auc(model_fpr, model_tpr)\n",
    "    \n",
    "        plt.figure(figsize = (8, 6))\n",
    "        plt.rcParams['font.size'] = 16\n",
    "        \n",
    "        # Plot both curves\n",
    "        plt.plot(base_fpr, base_tpr, 'k', label = 'baseline-0.5 AUC')\n",
    "        plt.plot(model_fpr, model_tpr, 'r', label = 'model-testing')\n",
    "        plt.legend(loc='lower right');\n",
    "        plt.xlabel('False Positive Rate'); \n",
    "        plt.ylabel('True Positive Rate'); \n",
    "        plt.title(r'SVM ROC Curves  (AUC = %0.2f)' % (results['roc_auc'])); \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        save_path = fold_path+'/AUC=%0.2f'%(results['roc_auc'])+ '.png'\n",
    "        \n",
    "        plt.close('all')\n",
    "    \n",
    "        \n",
    "        return results['roc_auc']\n",
    "        \n",
    "\n",
    "    def plot_feature_importance(self,classifier):\n",
    "        #plot feature importance and save it\n",
    "        if self.method=='XGBoost':\n",
    "            #TODO: make chart bigger, have to pass ax object here ..\n",
    "            xgb.plot_importance(classifier)\n",
    "            #plt.savefig(self.classif_method_path+'\\\\feature_importance.png')\n",
    "        elif self.method=='RF':\n",
    "            fi_model = pd.DataFrame({'feature': self.get_features(self.dataset),\n",
    "                   'importance': classifier.feature_importances_}).\\\n",
    "                    sort_values('importance', ascending = False)\n",
    "            #fi_model.to_csv(self.classif_method_path+'\\\\feature_importance.csv')\n",
    "\n",
    "    \n",
    "    def eval_outer_cv(self,outer_runs_details):\n",
    "        #compute avg of AUC scores and get best perfomring mmodel hparams\n",
    "        test_auc_scores = (outer_runs_details['test_auc'])\n",
    "        avg_test_auc = sum(test_auc_scores)/len(test_auc_scores)\n",
    "        scores_std = np.std(test_auc_scores)\n",
    "        #return best_params of best perfomring model on outer CV\n",
    "        best_score_index = test_auc_scores.index((max(test_auc_scores)))\n",
    "        best_hparams = outer_runs_details['best_inner_hparams'][best_score_index]\n",
    "\n",
    "        (pd.DataFrame(outer_runs_details)).to_csv(self.classif_method_path+'\\\\runs_details.csv') \n",
    "\n",
    "        return avg_test_auc,best_hparams,scores_std\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def load_top_10_features_data_AH():\n",
    "    '''utility fn for returning df with all BPAD200 proteins with their accession, sequence, \n",
    "    #top 10 features used by AH et al 2017 in top perfoming SVM (0.787 AUC) and class-labels'''\n",
    "    '''\n",
    "    @index_col: `None` means that a standard index col. will be used string \n",
    "    If string is given, then it indicates the column to select as index from loaded ones here\n",
    "    '''\n",
    "    proteins_filename = 'BPAD200_Balanced_Not_Scaled_Additional_Features_Protein_Sequences_Added - Copy - Copy.csv'\n",
    "    top10_feats=['LipoP_Signal_Avg_Length','YinOYang-T-Count','NetPhosK-S-Count','LipoP_SPI_Avg_Length','NetMhcPan-B-AvgRank','TargetP-SecretFlag','YinOYang-AvgDiff1','MBAAgl7_CorCount','PickPocket-Avg','PropFurin-Count_Score']\n",
    "    cols1 = ['Accession','Sequence','Length']\n",
    "    cols1.extend(top10_feats)\n",
    "    cols1.extend(['Label'])\n",
    "\n",
    "    prot_data = pd.read_csv(proteins_filename,\n",
    "                           names=cols1,index_col=None)\n",
    "                           \n",
    "\n",
    "    if len(prot_data)>400:\n",
    "        prot_data= prot_data[1::2]\n",
    "        prot_data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    return prot_data        \n",
    "\n",
    "\n",
    "\n",
    "def concat_dfs_list(dfs_list):\n",
    "    '''utility fn for concatenating dfs in a list efficiently, handling indexing of new df'''\n",
    "    #TODO: currently only works for creating dataframe with standard index, ie: not one were a different\n",
    "    #col. such as 'accession' will be used for index (or was used for index)\n",
    "    \n",
    "    dfs_list = [df.reset_index(inplace=False) for df in dfs_list]\n",
    "    dfs_list = [df.drop('index', axis=1,inplace=False) for df in dfs_list if 'index' in list(df.columns)]\n",
    "    df= pd.concat(dfs_list,axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_col_name_before_label_col(data):\n",
    "    '''for large datasets, prefer a functional one-liner to this fn'''\n",
    "    cols = list(data.columns)\n",
    "    label_ind = cols.index('Label')\n",
    "    col_before_label = cols[label_ind-1]\n",
    "    \n",
    "    return col_before_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''UTILITY FUNCTIONS'''\n",
    "\n",
    "def make_dir(paths):\n",
    "    '''creates directory path (or multiple directories if needed) if they \n",
    "    #don't exist and returns the path created\n",
    "    If dir. exists, then nth is done, just path is returned!\n",
    "    '''\n",
    "\n",
    "    if isinstance(paths,list):\n",
    "            \n",
    "        if len(paths)>2:\n",
    "            path = paths[0]\n",
    "            for i in range(1,len(paths)):\n",
    "                path = os.path.join(path,paths[i])\n",
    "            if os.path.exists(path) == False :\n",
    "                os.makedirs(path)\n",
    "                \n",
    "        elif len(paths)==2:\n",
    "            path = os.path.join(paths[0],paths[1])\n",
    "            if os.path.exists(path) == False :\n",
    "                os.makedirs(path)\n",
    "            \n",
    "        elif len(paths)==1:\n",
    "            path = paths[0]\n",
    "            if os.path.exists(path) == False :\n",
    "                os.mkdir(path= path)\n",
    "                \n",
    "        else:\n",
    "            raise ValueError('Length of list of paths has to be larger than 0')\n",
    "\n",
    "\n",
    "    elif isinstance(paths,str):\n",
    "        path = paths\n",
    "        if os.path.exists(path) == False :\n",
    "            os.mkdir(path= path)\n",
    "    \n",
    "    else:\n",
    "        raise TypeError('Argument @paths must be list of strings or a string, indicating the dir.path')\n",
    "    #all paths don't have '/' in the end!\n",
    "    return path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class for performing L.O.Bacteria.O.C.V. on best model & dataset combination, identified during model selection phase\n",
    "As of 2020 paper, best model: LR-PCA 11 PCs-standardized-\n",
    "    #using Classifier model class we do hparam tuning for each inner fold again for LOBOV\n",
    "'''\n",
    "\n",
    "class Lobov_model():\n",
    "    def __init__(self,base_path,model_dataset_path,\n",
    "                 split_method, outer_cv_splits,inner_cv_splits,classif_method,hparams):\n",
    "        \n",
    "        self.base_path=base_path\n",
    "        self.model_dataset_path = model_dataset_path\n",
    "        \n",
    "        self.entry={}\n",
    "        #self.hparams_model={'C': 0.01}\n",
    "        \n",
    "        self.hparams_models = hparams\n",
    "        self.load_data()\n",
    "        \n",
    "        self.inner_cv_splits = inner_cv_splits\n",
    "        self.outer_cv_splits = outer_cv_splits\n",
    "        self.classif_method= classif_method\n",
    "        self.split_method = split_method\n",
    "        \n",
    "        '''################CLASSIFICATION#########################################'''\n",
    "        #TODO: sort out duplication with main_model_selection.py\n",
    "        preProcessing_1 = getattr(self.entry,'preProcessing_1')\n",
    "        preProcessing_2 = getattr(self.entry,'preProcessing_2')\n",
    "        data_name = getattr(self.entry,'name')\n",
    "        dataset = getattr(self.entry,'dataset')\n",
    "            \n",
    "        dataset_path=make_dir([base_path,preProcessing_1,preProcessing_2,data_name]) \n",
    "        \n",
    "        Classifier_model_ = Classifier_model(self.classif_method,data_name,dataset,\n",
    "                                             dataset_path,preProcessing_1,\n",
    "                                             preProcessing_2,self.split_method,\n",
    "                                 self.outer_cv_splits,self.inner_cv_splits,\n",
    "                                 self.hparams_models)\n",
    "        lobov_data_dir = make_dir(['tut-3-lobov','bpad200_lobov_splits_indic'])\n",
    "        Classifier_model_.outer_cv(lobov_data_dir)\n",
    "\n",
    "        #no need to save results of runs again, outside of classif_method_path\n",
    "        \n",
    "    def load_data(self):\n",
    "        prot_cols = 'infer'\n",
    "        prot_ind=[0]\n",
    "        fetch = Load_data_model_selection(prot_cols=prot_cols,\n",
    "                                      prot_ind=prot_ind)\n",
    "        fetch.populate_data_df(self.model_dataset_path)\n",
    "        \n",
    "        data_df = fetch.get_all_data()\n",
    "        \n",
    "        #lobocv classify\n",
    "        #all_data_df has only 1 entry\n",
    "        for entry in data_df.itertuples(index=False,name='Pandas'):\n",
    "            self.entry = entry\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''MAIN SCRIPT FLOW '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SETTING PATHS'''\n",
    "#date_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "#tut_path = make_dir(['tut-3-lobov',])#date_time,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ARGUMENTS'''\n",
    "'''setting data Path'''\n",
    "\n",
    "'''CHANGE PATH ACCORDING TO TUT-2'''\n",
    "\n",
    "best_model_dataset_path = r'.\\tut-1-PCA-optimal-model\\pre_processing\\standardise\\PCA\\PCs.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying parameters for inner-CV that will be done for each species-fold (25 overall)\n",
    "hparams = {'C': [0.01]}\n",
    "#specifying that we will do a LOBOV outer validation instead of a standard nested 10-fold CV on BPAD200\n",
    "split_method = 'lobov'\n",
    "outer_cv_splits=10\n",
    "inner_cv_splits=10\n",
    "classif_method = 'LR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm = Lobov_model(tut_path,best_model_dataset_path,split_method,\n",
    "                        outer_cv_splits,inner_cv_splits,classif_method,hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''plot prototype bar plot LOBOV'''\n",
    "'''AUC scores not flipped here as in paper'''\n",
    "\n",
    "res = pd.read_csv(r'.\\tut-3-lobov\\standardise\\PCA\\PCs\\lobov\\LR\\runs_details.csv', index_col=[0])\n",
    "\n",
    "# species\n",
    "species = res.iloc[:,0].values.tolist()\n",
    "assert len(species)==25 and isinstance(species,list)\n",
    "#AUC\n",
    "auc = res.iloc[:,1].values.tolist()\n",
    "assert len(auc)==25 and isinstance(auc,list)\n",
    "#AH_AUC\n",
    "#received through personal communication with Heinson et. al 2019\n",
    "auc_ah = [0.94,0.89,1.00,0.83,0.75,0.78,0.44,0.92,0.74,0.94,0.73,1.00,0.61,0.56,0.79,0.50,0.94,0.69,0.52,0.55,0.81,0.69,0.84,1.00,0.58]\n",
    "assert len(auc)==25\n",
    "\n",
    "x = np.arange(len(species))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,12))\n",
    "rects1 = ax.bar(x - width/2, auc, width, label='AUC',color=(0, 0.4, 0, 1))\n",
    "rects2 = ax.bar(x + width/2, auc_ah, width, label='AUC_AH',color=(0, 0, 0, 1))\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('AUC Score')\n",
    "ax.set_title('LOBOV AUC')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(species,rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(tut_path+'\\\\LOBOV_bar_plot.png')\n",
    "\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LOBOV plot in the paper was build using Microsoft Excel and the data in the runs_details.csv file in the \"\\standardise\\PCA\\PCs_of_pca_with_11PCs\\lobov\\LR\" folder in the directory where this jupyter notebook is placed.\n",
    "\n",
    "The columns for the bacteria species, the LOBOV scores computed in this paper, the LOBOV scores from Heinson et. al 2019 were used from the file, as well as the size of each species dataset (each testing dataset) were used from the runs_details.csv file mentioned above. The latter was calculated during the production and formatting of the figure using Microsoft Excel. Also, as explained in the manuscript, any AUC scores below 0.5 were flipped on the LOBOV plot presented on the manuscript."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
